# cos-pomdp

COS-POMDP is a POMDP.
This package defines that POMDP and a planner to solve it.
This package also includes instantiation of this POMDP to Ai2Thor for object search.

How should a POMDP project be developed?
You define the POMDP. Then, instantiate it on your domain.
Do all the necessary state (e.g. coordinate) conversions.
You create one or more planner given that POMDP.
Then, you have a POMDP and a solver for it!

Installation
```
pip install -e .
```
This will install `cospomdp` which is the core code for the POMDP components,
and `cospomdp_apps` which contains application to the POMDP to several domains.

Test basic example:
```
python -m cospomdp_apps.basic.search
```

### Caveats
The external methods, e.g. SAVN, MJOLNIR, are placed under `cos-pomdp/external`.
However, for importability, a symbolic link to the `cos-pomdp/external/mjolnir`
directory is created under `cos-pomdp/cospomdp_apps/thor/mjolnir`. Please
make sure that link points to the right absolute path on your computer.
For example, you can directly create a new one by:
```
cd repo/cos-pomdp/cospomdp_apps/thor
ln -sf $HOME/repo/cos-pomdp/external/mjolnir/ mjolnir
```

## Organization
Contains two packages: `cospomdp` and `cospomdp_apps`.
The former defines the COS-POMDP (domain, models, agent, planner)
and the latter applies COS-POMDP to specific applications
such as ai2thor.

## YOLOv5 data organization
```
cos-pomdp/data/
    yolov5/
        train/
            images/
                FloorPlan10-img0.jpg
                ...
            labels/
                FloorPlan10-img0.txt
                ...
```

## Create data
```
python -m cospomdp_apps.thor.data.create data/yolov5 --num-train-samples 3 --num-val-samples 1
```
Note that the script may generate slightly more samples than `num-train-samples`
and `num-val-samples` because the data is generated by randomly placing the robot
then looking in all directions; Image from all directions will be collected for
any agent placement.

To create the dataset for scenes separately, used to train our
object detectors, do:
```
cd scripts
source gather_detector_training_data.sh
```


## Browse generated data
```
python -m cospomdp_apps.thor.data.browse -m yolo path/to/dataset/yaml
```
Use `a` and `d` to browse back and forth. To quit, press `q`.

## Train YOLOv5
https://github.com/ultralytics/yolov5

You may need to install:
```
pip install tensorboard
```

Train the model with
```
source train_yolov5.sh
```
You want to make sure the dataset directory is of valid format.
The best way to do that is to run the dataset creation step
above till finish.

The current script trains one model per scene category.

Note the `val_yolov5.sh` script is useless. See [this github issue](https://github.com/ultralytics/yolov5/issues/4199).
All the results that YOLOv5 generates are for the validation datasets already.
To run the model on validation dataset:
```
cd scripts
python val_vision_detector.py ../models/yolov5-kitchen/best.pt ../data/yolov5-kitchen/yolov5-kitchen-dataset.yaml kitchen
```

## Try out YOLOv5 in action
```
cd scripts
python run_vision_detector_in_action.py ../models/yolov5-kitchen/best.pt ../data/yolov5-kitchen/yolov5-kitchen-dataset.yaml kitchen
```


## AI2-Thor Setup

Compare with:
- [1] IQA (CVPR'18)
- [2] Visual Semantic Navigation Using Scene Priors (ICRL'19)
- [3] Learning hierarchical relationships for object-goal navigation (CoRL'20)
- [4] Hierarchical and Partially Observable Goal-driven Policy Learning with Goals
  Relational Graph (CVPR'21)


|                  | grid size | h_rotate | v_rotate | fov |
|------------------|-----------|----------|----------|-----|
| [1] IQA          | 0.25      | 90       | 30       | 60  |
| [2] Scene Priors | 0.25      | 45       | 30       | 90  |
| [3] MJONIR       | 0.25      | 45       | 30       | 100 |
| [4] HRL-GRG      | 0.25      | 90       | 30       | 90  |
| [5] ALFRED       | 0.25      | 90       | 15       | 90  |
| ours             | 0.25      | 45       | 30       | 90  |

## Dependencies

Check out requirements.txt. This is created under
the `cosp` virtual environment (07-24-2021).
